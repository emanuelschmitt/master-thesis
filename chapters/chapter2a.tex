\chapter{Machine Learning Fundamentals}
As machine learning techniques will be used for the classification of individual tap locations on a smartphone touchscreen, the following chapter will give a brief overview of the fundamental concepts evolving around statistical learning. Furthermore, the methods that going to be applied to the acquired sensor data will be discussed.

\section{Overview and Definition}

Ever since computers were invented, there has been a desire to enable them to learn \cite{samuel2000some}. This desire has grown into the field of machine learning which seeks to answer questions on how to build build systems that automatically improve with experience, and what the fundamental laws of learning processes are. Today, state-of-the-art ML covers a large set of methods and algorithms designed to accomplish tasks where conventional hard-coded routines have brought insufficient results. From speech recognition to email spam detection or recommendation systems, ML methods find broad usage in a variety of problem domains. \\

In order to understand what the principle of machine learning is, we will start with a definition by Samuel \cite{samuel2000some}:\\
\begin{quote}
\textit{Machine learning is the field of study that gives computers the ability to learn without being explicitly programmed.}\\
\end{quote}

In this definition, special emphasis is to be put on the last part of this definition. A computer is only then able to learn when he can perform a task without being explicitly instructed. Thus, in order to learn, the computer must somehow be able to instruct itself without the influence of an outer . As this definition lacks a more detailed view on what computer learning is, we will dive into a definition by Tom Mitchell \cite{mitchell2006discipline}:\\
\begin{quote}
\textit{A learning system is said to learn from experience E with respect to some class of tasks T and performance measure P, if its performance at tasks in T, as measured by P, improves with experience E.}\\
\end{quote}

The example that Mitchell notes, is one from the games of checkers \cite{mitchell2006discipline}. In this case checkers is the task T that the computer is aiming to learn. In order for the computer to learn, information on previously played matches is required. Since the computer does not know how to evaluate is a particular match was either good or bad, we set the performance to be defined based on how many matches were actually won. If a computer program can raise the amount of games won \textit{(performance measure P)} with the help of the experience from previously placed matches \textit{(experience E)} then it can learn to play checkers \textit{(task T)}. \\

% TODO: Maybe delete this.
To break this down into a more practical perspective, the challenge lies in finding an appropriate model in order to learn from data, which is the most common format to represent past experience. By learning, the computer adjusts parameters on the model based on the data that we feed the system with. Once the model has been adjusted, it can perform tasks with new incoming experience.

\section{Categorization of Methods}
As machine learning algorithms and methods differ from their approach to learning and underlying concepts, it is common practice to separate these into the following categories \cite{Duda:2000:PC:954544, Marsland:2009:MLA:1571643}  : Supervised learning, unsupervised learning, reinforcement learning and evolutionary learning. In the following sections I will briefly outline these.\\

\begin{itemize}

  \item[] \textbf{Supervised learning}, which is also named learning from example, is presumably the most prominent category of ML algorithms. The algorithm is given a training set of examples $\{x_0, \dots, x_n \}$, which are also known as \textit{features} and the correct target values $\{y_0, \dots, y_n \}$ mapped to each set of features, which is the answer that the algorithm should produce. The algorithm then generalizes based on the training set in order to respond with sensible outputs on all possible input values. Outputs, if they are discrete labels, correspond to a classification task whereas outputs on a continuous scale refer to a regression task (see \cite{Marsland:2009:MLA:1571643}).

An example for supervised learning is the classification of malignant or benign tumors as seen in cancer diagnosis. Let's assume we have a dataset with different properties of a tumor, such as the size or the color of the cells. These properties form our features $x$. Each set of features is mapped to an output label $y$ stating if the tumor is malignant or benign. The first step is to use the pairs $(x, y)$ of the training set to teach the algorithm the correct mapping of the problem space. As $x$ is linked to the output $y$ in the training set, learning the conjunction of these two values is done under supervision since the output label $y$ is given. Once learned, the algorithm is generalized to map unseen inputs to the correct output label.

Practical applications are for example digit and handwriting recognition \cite{lecun1990handwritten}, spam filtering \cite{guzella2009review} for e-mails or network anomaly detection \cite{lee2010uncovering}.

Presumably the most widely known machine learning techniques belong to this category, such as Support Vector Machines (SVMs), Artificial Neural Networks, Bayesian Statistics, Random Forests and Decision Trees \cite{Duda:2000:PC:954544}.\\

%TODO: add referation to how we will solve issues in the thesis.

  \item[] \textbf{Unsupervised learning} is the task of learning structures from input values that are not explicitly labeled. In comparison to supervised learning, where correct output values are provided for each input, unsupervised algorithms learn to identify similarities in the input data and can therefore group these \cite{Duda:2000:PC:954544}. These grouping problems are referred to as \textit{clustering}. The underlying idea here, is that humans learn by not explicitly being told what the right answer should be \cite{Marsland:2009:MLA:1571643}. If a human sees different species of snakes, for instance, he or she is able to identify them all as snakes. Hence, the human is aware that there are differences in each specific type of snake without specifically knowing a correct label.

A prominent example where unsupervised learning is heavily used, is in recommender systems for online retail shops. Amazon.com, for instance, uses a technique called \textit{collaborative filtering}, which measures similarity in customers based that they have previously bought \cite{linden2003amazon}. Having identified similar customers utilizing the cosine similarity, the algorithm can then recommend items that similar users have bought. This technique is also used for music recommendations \cite{perez2017recommender} or social network recommendations \cite{kautz1997referral}.

The field of unsupervised learning is closely related to density estimation in statistics, as with the density of inputs, we are able to group them. The K-means algorithm is the most prominent in this field \cite{Marsland:2009:MLA:1571643}.\\
  
  \item[] \textbf{Reinforcement learning} falls in between supervised and unsupervised learning methods. Whereas supervised learning tries to bridge the gap between input and corresponding output values and unsupervised methods detect groupings in incoming data, reinforcement learning is based on learning with a \textit{critic} \cite{Marsland:2009:MLA:1571643}. The algorithm tries different solution strategies to a problem and is told weather or not the answer provided was correct. An important fact here, is that the algorithm is not told how to correct itself. This practice of "trying-out" is based on the concept of \textit{trail-and-error learning} which is known as the \textit{Law-of-effect} \cite{Marsland:2009:MLA:1571643}. A good example is a child that tries to stand up and learn walking. The child tries out many different strategies for staying upright and receives feedback from the field based on how long it can stand without falling down again. The method that previously worked best is then repeated in order to find the optimal solution resulting in the child learning to walk \cite{Marsland:2009:MLA:1571643}.
  
 In more mathematical terms, the reinforcement learning problem is formalized with an agent and his environment. The environment in which the agent is set provides a set of \textit{states} on which the agent can perform \textit{actions} to maximize a certain \textit{reward}. By performing actions the state changes and a new reward is calculated. The reward then tells the agent if the action was a good choice. Goal of the algorithm is to maximize the reward \cite{Marsland:2009:MLA:1571643}.
 
 Reinforcement learning is a practical computational tool for constructing autonomous systems that improve themselves with experience. These applications have ranged from robotics, to industrial manufacturing, to combinatorial search problems such
as computer game playing \cite{kaelbling1996reinforcement}. 
Prominent methods of this category are Q-learning, Monte Carlo
methods and Hidden Markov Models \cite{Marsland:2009:MLA:1571643}.\\

  \item[] \textbf{Evolutionary learning} is inspired by strongly inspired by nature. As biological evolution improves the survival of a species, the strategy of adaptation to improve survival rates and the chance of offspring has inspired researchers to craft genetic algorithms (GA) \cite{Marsland:2009:MLA:1571643}. 

Genetic algorithms are a family of adaptive search procedures which have derived their name from the fact that they are based on models of genetic change in a population of individuals. These models have their foundation in three basic ideas: (1) Each evolutionary state of a population can be evaluated on a \textit{fitness} scale. This is done since biological evolution has a natural bias towards animals that are \" fitter \" than others. These animals tend to live longer, are more attractive and generate healthier and happier offspring, an idea which was originated in Charles Darwin's \"The Origin of Species\". (2) Each population can be mated to generate offspring using a \textit{mating operator}. (3) The third component are \textit{genetic operator}, such as \textit{crossover} and \textit{mutation}, which determine how the offspring solution is composed of the genetic material of the parents \cite{de1988learning}.

Evolutionary learning is often considered when other methods fail to find a reasonable answer. Algorithms find applications in search and mathematical optimization, but also in arts and simulation \cite{Marsland:2009:MLA:1571643}.\\
\end{itemize}

In this section we have seen several different problems that we can solve with the help of algorithmic learning. For our use case, as we want to predict the locations on smartphone screens using sensory data. As this is a supervised learning problem, we will cover one supervised approach in more detail in the following section: Artificial neural networks.

\section{Support Vector Machines}

A SVM is a non-linear kernel based extension of the so-called maximum margin classifier. Originating from binary classification problems, were $y \in \{1, -1\}$, the general idea of a maximum margin classifier is to find a separating hyperplane in the p-dimensional feature space. This hyperplane separates the training examples leading to a maximum distance between the observations of the two classes \cite{James:2014:ISL:2517747}. This distance is referred to as margin $M$ measuring the smallest distance of a training observation towards the defined hyperplane. In order to find a suitable hyperplane only a subset of the training examples are sufficient.\\

Despite all that, linear separability is unlikely to be applicable to most real world problems. However, when a problem is linear separable, the constraint to the support vectors induce an undesirable sensitivity to individual observations, which can lead to high variance of the classifier. For this reason, the maximum margin classifier can be extended to it's more robust successor, the support vector classifier. Still being a linear classifier, the SVM allows for violations in fitting the margin. \\

Mathematically, the support vector classifier can be described via following optimization problem \cite{James:2014:ISL:2517747}:
\begin{eqnarray}
  & \max_{\beta, \epsilon} M \\
  & \textrm{subject to} \sum_{s=1}^{p} \beta^2_s = 1 \\
  g(x_{i}) &= y_i(\beta_0, \beta_1 x_1, \dots \beta_p x_p) \geq M(1 - \epsilon) \\
  & \epsilon \geq 0, \sum_{i=1}^n \epsilon_i \leq C
\end{eqnarray}

The objective of this optimization problem is to maximize the margin $M$ while choosing appropriate vector parameters $\beta$ and $\epsilon$. In this context, the parameter vector $\beta$ contains the coefficients of the hyperplane and the vector $\epsilon$ includes so-called slack variables that account for instances which are located on the wrong side of the margin and the hyperplane. These can be expressed as follows assuming that M is positive \cite{James:2014:ISL:2517747}:
\begin{equation}
  \epsilon_i =
  \begin{cases}
    0 & g(x_i) \geq M \\
    > 0 & M < g(x_i) < 0 \\
    > 1 & g(x_i) > 0 
  \end{cases}
\end{equation}

The hyperparameter C is a non-negative variable, which can be seen as a budget variable that allows for a certain sum of $\epsilon_i$ observations to be on the wrong side of the margin or hyperplane, respectively \cite{James:2014:ISL:2517747}. C manages the bias-variance tradeoff, since a low C tries to find a maximum margin hyperplane that almost exactly separates the two classes, resulting in low bias classifier for the available data set but in a high variance classifier for test data. Concurrently, allowing high budget C results in a high bias classifier that widens the margin, introducing more violations $\epsilon_i$ while reducing the variance of the classifier (Ibid.). Thereby C also controls the number of considered support vectors in dependence of the margin width.\\

Extending the support vector classifier to non-linear decision boundaries brings us to the SVM. Instead of extending the predictor space using higher order polynomials and interactions, SVM uses the so called ``kernel trick'' \cite{efron_hastie_2016}. In order to apply the ``kernel trick'' , i.a. \citeauthor{efron_hastie_2016} show that the optimization problem above can be rewritten as
\begin{equation}
  \^{f}(x) = \beta_0 + \sum_{i \in S} \alpha_i \langle x, x_i \rangle
\end{equation}

where $i \in S$ defines the subset of support vectors and $\langle x, x_i \rangle$ is the dot product of all pairs in the support vector. Thus, the parameters $\beta_0$ and $\sum_{i \in S}$ can be estimated with the help of least squares via simply computing the inner products of each pair in the support vector \cite{efron_hastie_2016}. The Expression in $\langle x, x_i \rangle$ can be generalized by a kernel function
\begin{equation}
  K(x_i,x_{i'}) = \sum_{s=1}^{p} x_{is} x_{i's}
\end{equation}
indicating the linear kernel that quantifies the distance between each pair in the data set \cite{James:2014:ISL:2517747}. Accordingly, the equation above can be rewritten as
\begin{equation}
  \^{f}(x) = \beta_0 + \sum_{i \in S} \alpha_i K(x, x_i)
\end{equation}
but in spite of restricting $K(\cdot, \cdot)$ to (3.7), we can now choose an arbitrary kernel function that maps our data into a high dimensional space where it is linearly separable. With this ``kernel trick'' the SVM can work in an implicitly enlarged predictor space, just by computing $\binom{n}{2}$  kernel functions $K(\cdot, \cdot)$, as opposed to an explicitly augmented predictor space, which is in fact computationally intractable \cite{James:2014:ISL:2517747}. In this work, we will be using a radial kernel function:
\begin{equation}
  K(x_i,x_{i'}) = \exp(-\gamma(\sum_{s=1}^{p}x_{is}x_{i's})^2),
\end{equation}
where $\gamma$ is a tuning parameter. After the parameters are learned on the basis of the training set, a new observation with the feature vector $x_0$ is classified via the following decision rule
\begin{equation}
  \^{f}(x) = sign(\beta_0 + \sum_{i \in S} \alpha_i K(x_i, x_{i'})).
\end{equation}
In view of the task at hand, an extension to multi-class classification of the SVM is utilized via one-versus-one classification. Given C classes, $\binom{C}{2}$ binary classifiers are learned in such a fashion that the C-th class is coded as $+1$ and the C0-th class as $1$. Consequently, a new instance is assigned to the class to which it was classified most frequently.
\newpage
\section{Neural Networks}

\subsection{Neurons}

A neuron is fundamental processing unit of an artificial neural network. The basic elements of a neuron are as follows \cite{Haykin:1998:NNC:521706}:
\begin{itemize}
  \item A set of \textit{connecting links} or \textit{synapses} which have a certain weight defined as the vector $\vec{w}$. The signal, represented as $\vec{x}$, flows through the \textit{synapse} and is mutipied by it's weight $w_i$.
  \item A \textit{adder} for summing the input signals and weights of the incoming synapses. These operations constitute a linear combiner.
  \item An \textit{activation function} $\varphi$ for limiting the amplitude of the output signal. This function is often referred to as the \textit{squashing function} since it squashing the possible output range.
  \item An externally applied \textit{bias} $b$ which has the ability to lower or rise the net input to the activation function depending if the bias is negative or positive.
\end{itemize}
\begin{figure}[h]
	\centering
	\begin{tikzpicture}
    \node[draw,circle,minimum size=25pt,inner sep=0pt] (x) at (0,0) {$\Sigma$ $\varphi$};
    
      \node[inputNode] (x0) at (-3, 1.5) {$\tiny x_0$};
      \node[inputNode] (x1) at (-3, 0.75) {$\tiny x_1$};
      \node[inputNode] (x2) at (-3, 0) {$\tiny x_2$};
      \node[inputNode] (x3) at (-3, -0.75) {$\tiny x_3$};
      \node[inputNode] (xn) at (-3, -1.75) {$\tiny x_n$};

      \node[inputNode] (b) at (-0.12, 2) {$\tiny b_k$};
    
      \draw[stateTransition] (x0) to[out=0,in=120] node [midway, sloped, above=-2] {$w_{k0}$} (x);
      \draw[stateTransition] (x1) to[out=0,in=150] node [midway, sloped, above=-2] {$w_{k1}$} (x);
      \draw[stateTransition] (x2) to[out=0,in=180] node [midway, sloped, above=-2] {$w_{k2}$} (x);
      \draw[stateTransition] (x3) to[out=0,in=210] node [midway, sloped, above=-2] {$w_{k3}$} (x);
      \draw[stateTransition] (xn) to[out=0,in=240] node [midway, sloped, above=-2] {$w_{kn}$} (x);
      \draw[stateTransition] (x) -- (2,0) node [midway,above=-0.cm] {$\tiny y_k$};
      \draw[stateTransition] (b) -- (-0.12,0.44) node [midway, above=-0cm] {};
      \draw[dashed] (0,-0.43) -- (0,0.43);
      \node (dots) at (-3, -1.15) {$\vdots$};
  \end{tikzpicture}
\end{figure}

Mathematically, a neuron $k$ can be expressed with the following equation \cite{Haykin:1998:NNC:521706}
\begin{eqnarray}
  u_k &= \sum_{j = 1}^{m}w_{kj}x_{j} \\
  y_k &= \varphi(u_k + b_k)
\end{eqnarray}
where $x_1, \dots, x_n$ are the input signals; $w_{k1}, \dots, w_{kn}$ refer to the synaptic weights of the neuron; $u_k$ is the linear combiner output of the summation on which the bias $b$ is added. The output of the neuron is expressed as $y_k$.

\subsection{Activation Function}
\subsection{Feed-forward network}
