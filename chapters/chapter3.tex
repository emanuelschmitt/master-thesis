\chapter{Data Aquisition System\label{cha:chapter3}}
For labeled data acquisition a system was required that is able to function in a laboratory environment, as well as recieve the data coming from the field study. For this purpose TapSensing was created. TapSensing is an iOS application that collects touch events including their sensory information to then send them to a backend server application. In the following sections we will outline the different components of TapSensing. 

- Introduce TapSensing
- Introduction of the chapter

\section{Overall System Architecture}

The TapSensing application consists of two main components: the mobile and the server-side application. In brief, the mobile client provides the ability for a user to generate tap information including the motion sensor recordings. For the data to be stored in a centralized manner, the server-side application provides HTTP endpoints as a gateway to the database. The architecture, as illustrated in figure \ref{fig:architecture}, consists of various components which are outlined in the following. \\

\begin{figure}[h!]
  \centering
  \includegraphics[width=0.7\textwidth]{architecture.png}
  \caption{The diagram shows the overall architecture of TapSensing.} \label{fig:architecture}
\end{figure}

\begin{itemize}
  \item \textbf{Mobile application}: The mobile application provides interfaces for the user to generate taps with corresponding sensor information. Furthermore, the application is capable of sending the acquired data to the backend application. More features of the mobile application are presented in section \ref{sec:mobileapp}. % # TODO
  \item \textbf{NGINX}: For accepting and routing incoming HTTP requests, an NGINX reverse proxy/load balancer is used. The reverse proxy forwards the incoming requests to the server-side application and is capable of serving static content, such as images, HTML, CSS and JavaScript files of the form application.
  \item \textbf{Gunicorn}: Gunicorn is a Python Web Server Gateway Interface (WSGI) HTTP server which runs the source code of the backend application.
  \item \textbf{Backend Application}: The backend application provides HTTP API endpoints which inherit the server-side logic. The backend provides authentication and persistence functionalities. More information on the backend application is to be found in section \ref{sec:backend}
  \item \textbf{PostgreSQL}: TapSensing uses a PostgreSQL\footnote{PostgreSQL is a general purpose and object-relational database management system.} database for storing the application state, user related information, the survey data and the retrieved tap and sensor information.
  \item \textbf{Form application}: The form application provides a web user interface where study participants can answer survey questions.
\end{itemize}

% For the network requests containing the tap information which come from the mobile device to reach the backend, it must first pass through a reverse proxy. As reverse proxy we have chosen NGINX due to it's easy configurability. In this case, NGINX forwards requests to the TapSensing application and serves static files.\\
% The TapSensing backend is written upon the Python Django Framework\footnote{https://www.djangoproject.com/} which is being executed upon the gunicorn application server. Django uses a so-called ORM to perform transactions with the Database, which in our case is a PostgreSQL database.

\section{Mobile Application}
Mobile
\label{sec:mobileapp}

\subsection{User Interface}
\subsubsection{Login}
When the application is opened for the first time, a login screen appears. As in standard login screens, the interface asks for credentials including username and password. Authenticating users, has the advantage, that the generated data can be mapped to individual users.\\

The source code of the of the question view is to be found in \texttt{LoginViewController.swift}.

\subsubsection{Start Screen}
During the trial the user is asked to generate taps once a day. In order to indicate if the user is eligible to perform a tap generation trial, the start screen shows a button that is either active or inactive. This switch depends on 4 distinct conditions: \\

When data is collected in the laboratory environment, the app is set to \textit{lab mode}. In \textit{lab mode} the button is always active and trials can be performed. When a user has not performed a trial today, the button is inactive. In contrast, when a user has performed a trial today, the button is inactive and a further trial can be performed the next day. Once all field trials are performed, the app confirms that all data is collected and the button remains inactive.\\

The source code of the of the question view is to be found in \texttt{StartViewController.swift}.
\subsubsection{Tap Input}

\begin{figure}[h!]
  \centering
  \includegraphics[width=0.8\textwidth]{grids_iphone.png}
  \caption{The figure shows the tap input user interface with buttons aligned in a grid shape structure. The leftmost structure offers 4 buttons, the middle offers 12 buttons whereas displays 20 distinguishable buttons.}\label{fig:grid}
\end{figure}

To acquire individual user taps the mobile application offers a user interface where buttons are aligned in a grid shape structure. The structure is calculated based on a specific configuration set where the amount the vertical and horizontal buttons in the interface can be set. Figure \ref{fig:grid} shows the interface, where 4, 12 and 20 distinguishable buttons are configured.\\

For the user to tap on every location of the screen exactly once, a red button indicating the next button to tap is highlighted guiding the user through the interaction process. While the user is tapping the grid, the gyroscope and accelerometer information is recorded. After all buttons gave been tapped, either a new grid is loaded or the tap acquisition phase ends proceeding with the question interface.\\

% Maybe a table
% \begin{itemize}
% \item 2 x 2
% \item 4 x 3
% \item 4 x 5
% \end{itemize}

% \begin{center}
%     \begin{tabular}{ | l | l | l | }
%     \hline
%     \# & height & width \\ \hline
%     1 & 2 & 2 \\
%     2 & 4 & 3 \\
%     3 & 4 & 5 \\
%     \hline
%     \end{tabular}
% \end{center}

The source code of the of the tap input view is to be found in \texttt{GridViewController.swift}.


\subsubsection{Questions}
To furthermore label the data acquired in the tap input interface, the application provides screens for the user to answer several questions. These question regard the various condition in which the user has generated taps. In the table below the asked questions with corresponding answer choices are to be found.

\begin{center}
  \begin{tabular}{| l | l | l | l |}
  \hline
  \textbf{Question} & \textbf{Answer Choices} \\ \hline
  Which body posture was used during the interaction ? & Standing, Sitting \\
  Which finger did you use while tapping? & Index Finer, Thumb \\
  Which hand did you use to tap? & Left, Right \\
  What is your current mood? & 1 - 5 \\
  \hline
  \end{tabular}
  \caption{The Table shows the questions asked in the question view.}
\end{center}

To enable fast interactions, each answer possibility to a question is represented in the interface with an icon. Once an icon has been pressed, the application transits to the next question until all questions have been answered.\\

\begin{figure}[h!]
  \centering
  \includegraphics[width=1\textwidth]{questions_iphone.png}
  \caption{The figure displays question views with icons as answer possibilities.}
\end{figure}

The source code of the of the question view is to be found in \texttt{QuestionViewController.swift}.

\subsubsection{Upload}
After all taps and questions are gathered, the acquired data is sent to the server. The interface at this points displays a spinning wheel for the user to acknowledge that the mobile phone is processing the data. In case an upload fails, the application provides a manual upload screen, where past sessions can be uploaded.

% \subsection{Smartphone Sensors}
% Modern smartphones come with a variety of different sensors offering valuable services to it's users and enhancing many applications. The newest Apple iPhone to date, the iPhone 7, has a fingerprint sensor, a barometer, a three-axis gyroscope and accelerometer (MEMS), a proximity sensor and an ambient light sensor attached to it's main-board \cite{iphone7techspecs}. As we are going to predict finger taps on the iPhone screen, the only sensors that are effected by the force of the tap are the gyroscope and the accelerometer. Therefore, these will be outlined in the following sections.

% \subsubsection{Accelerometer}

% The accelerometer is a sensor module that measures the acceleration it encounters by either movement or gravity \cite{sensorsstudy}. However, the acceleration caused by movement, the so-called inertial acceleration and the gravitational acceleration can not be distinguished by the sensor. This is due to Einstein's equivalence principal stating that the effects of gravity on an object are indistinguishable from the acceleration of the object's reference frame \cite{doi:10.1021/ed014p49.2}. \\

% \begin{figure}[h!]
%   \centering
%   \includegraphics[width=0.35\textwidth]{iphone-acc.png}
%   \caption{Apple iPhone with the corresponding axes of the accelerometer.}
% \end{figure}

% When the position of the device is fixed, as for example when it is placed on a table the accelerometer values would yield $a = \{ a_x = 0, a_y = 0, a_z = -1\}$. This feature make it suitable for detecting device screen rotations. As the device flips from landscape to portrait orientation, the gravity is sensed by a different set of accelerometer axis \cite{sensorsstudy}.\\

% The values of the acceleration are quantified in the SI unit metres per second per second ($m/s^2$). However, in engineering the acceleration is typically expressed in terms of the standard gravity ($g$).

% \subsubsection{Gyroscope}

% As the accelerometer is suitable for detecting orientations, it lacks the ability to detect spin or more precise rotation movements. These spin movements are detected by the gyroscope sensor which is responsible for detecting and maintaining orientation \cite{sensorsstudy}. \\

% A mechanical gyroscope typically composes of a spinning wheel which is set within three so-called gimbals. These gimbals enable the spinning wheel to be set in any orientation. Although the orientation does not remain fixed when the device is rotated, it changes in response to an external torque much lesser and in a different direction than it would be without the large angular momentum associated with it. Each gimbals translates to one of the three gyroscope outputs, namely \textit{pitch}, \textit{roll} and \textit{jaw} (See \cite{wiki:Gyroscope}). \\

% The gyroscope sensor within the MEMS\footnote{Microelectromechanical systems}, the chip deployed in the iPhone, is between 1 to 100 micrometers of size. When the gyroscope is rotated, a small resonating mass is shifted as the angular velocity changes. This movement is converted into very low-current electrical signals that can be amplified and read by a host system. \\

% The values of the gyroscope are quantified in as rotations per seconds (RPS) or as degrees per second ($\deg/s$).

% \subsubsection{Accessing sensor values}
% % Apple offers api for accessing these values.
% In order to access gyroscope and accelerometer Apple provides a high level API\footnote{An Application Program Interface is a set of rules and subroutines provided by an application system for the developer to use. Here is a link to the Core Motion API Documentation: \url{https://developer.apple.com/documentation/coremotion/} } for accessing the device's sensors: \texttt{Core Motion}. Core Motion reports motion and environmental related data from sensors including accelerometers, gyroscopes, pedometers, magnetometers, and barometers in easy to use manner. \\

% Sensor values can either be accessed as proceeded version including aggregations of the values and a raw version. For TapSensing, we make sure to record the raw values to avoid any form of bias. The update interval can be configures at ranges from $10$Hz - $100$Hz. Higher update-rates are possible but are not ensured to be processed in real-time by the device. For TapSensing, the update rate is configured with the highest (safe) value possible. This ensures that tap patterns are captured with an accurate resolution to make a later classification easier. The figure below is a code snipping depicting how sensor values are retrieved in the TapSensing application.

% \begin{figure}[thp]
% \centering
% \begin{minipage}{0.7\textwidth}

% \begin{minted}{swift}
% let UPDATE_INTERVAL = 0.01
% let motionManager = CMMotionManager()

% // Set the update interval on both sensors
% motionManager.gyroUpdateInterval = UPDATE_INTERVAL
% motionManager.accelerometerUpdateInterval = UPDATE_INTERVAL

% // Start reading the sensor values
% motionManager.startAccelerometerUpdates()
% motionManager.startGyroUpdates()

% // Pass in a function to handle sensor update values,
% // which arrive in the update interval rate.
% motionManager.startAccelerometerUpdates(to: .bg) {
%     (data: CMAccelerometerData?, error: Error?) in
%     // handle incoming data
%     ...
% }
% motionManager.startGyroUpdates(to: .bg) {
%     (data: CMGyroData?, error: Error?) in
%     // handle incoming data
%     ...
% }
% \end{minted}
% \end{minipage}
% \caption{Swift code snippet displaying how to access sensor values with Core Motion.}
% \label{test}
% \end{figure}

\section{Backend application}
\label{sec:backend}
Backend